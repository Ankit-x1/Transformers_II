{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps Transformer Pipeline - Production Execution\n",
    "\n",
    "This notebook executes the complete MLOps pipeline for sensor failure prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 1: Setup\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from utils.config import Config\n",
    "from utils.logging import MLOpsLogger\n",
    "from utils.reproducibility import set_seed, get_device\n",
    "from data.dataset import create_dataloaders\n",
    "from models.transformer import TransformerClassifier\n",
    "from training.trainer import Trainer\n",
    "import torch\n",
    "import pickle\n",
    "import mlflow\n",
    "\n",
    "config = Config('../configs/base.yaml')\n",
    "logger = MLOpsLogger(\"main\", log_dir=\"../logs/training\")\n",
    "set_seed(config['data']['random_seed'])\n",
    "device = get_device()\n",
    "\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"MLOPS TRANSFORMER PIPELINE - PRODUCTION EXECUTION\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.log_config(config._config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 2: Data Preparation\n",
    "logger.info(\"Creating dataloaders...\")\n",
    "train_loader, val_loader, test_loader, scaler = create_dataloaders(\n",
    "    config._config,\n",
    "    batch_size=config['training']['batch_size']\n",
    ")\n",
    "\n",
    "logger.info(f\"Train batches: {len(train_loader)}\")\n",
    "logger.info(f\"Val batches: {len(val_loader)}\")\n",
    "logger.info(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Save scaler for deployment\n",
    "with open('../models/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 3: Build Model\n",
    "logger.info(\"Building Transformer model...\")\n",
    "model = TransformerClassifier(\n",
    "    n_features=config['data']['n_features'],\n",
    "    d_model=config['model']['transformer']['d_model'],\n",
    "    num_heads=config['model']['transformer']['num_heads'],\n",
    "    num_layers=config['model']['transformer']['num_layers'],\n",
    "    d_ff=config['model']['transformer']['d_ff'],\n",
    "    n_classes=2,\n",
    "    dropout=config['model']['transformer']['dropout']\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "logger.info(f\"Model parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 4: Train Model\n",
    "logger.info(\"Starting training...\")\n",
    "trainer = Trainer(model, config, device, logger)\n",
    "best_f1 = trainer.fit(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    experiment_name=config['mlflow']['experiment_name']\n",
    ")\n",
    "\n",
    "logger.info(f\"Training complete. Best validation F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 5: Hyperparameter Optimization (Optional)\n",
    "if config['automl']['enabled']:\n",
    "    logger.info(\"Starting AutoML hyperparameter search...\")\n",
    "    from automl.optuna_optimizer import run_hyperparameter_search\n",
    "    \n",
    "    best_params = run_hyperparameter_search(\n",
    "        config._config,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        device,\n",
    "        n_trials=config['automl']['n_trials']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 6: Final Evaluation\n",
    "logger.info(\"Evaluating on test set...\")\n",
    "model.load_state_dict(torch.load('../models/saved/best_model.pth'))\n",
    "test_metrics = trainer.validate(test_loader)\n",
    "\n",
    "logger.info(\"TEST SET RESULTS:\")\n",
    "logger.log_metrics(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 7: Model Export\n",
    "logger.info(\"Exporting model for deployment...\")\n",
    "\n",
    "# Export to ONNX\n",
    "dummy_input = torch.randn(1, 100, 4).to(device)\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"../models/onnx/model.onnx\",\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    ")\n",
    "\n",
    "logger.info(\"✓ Model exported to ONNX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 8: Register Model in MLflow\n",
    "mlflow.set_experiment(config['mlflow']['experiment_name'])\n",
    "with mlflow.start_run():\n",
    "    mlflow.pytorch.log_model(model, \"production_model\")\n",
    "    mlflow.log_metrics(test_metrics)\n",
    "    \n",
    "    # Register model\n",
    "    model_uri = f\"runs:/{mlflow.active_run().info.run_id}/production_model\"\n",
    "    mlflow.register_model(model_uri, \"sensor_failure_model\")\n",
    "\n",
    "logger.info(\"✓ Model registered in MLflow Model Registry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 9: Drift Monitoring\n",
    "logger.info(\"Setting up drift monitoring...\")\n",
    "\n",
    "# Get reference data (training set)\n",
    "X_ref = train_loader.dataset.X.numpy()\n",
    "X_ref_flat = X_ref.reshape(-1, config['data']['n_features'])\n",
    "\n",
    "from monitoring.drift_detector import DriftDetector\n",
    "drift_detector = DriftDetector(X_ref_flat, threshold=0.05)\n",
    "\n",
    "# Simulate production data (use test set as example)\n",
    "X_prod = test_loader.dataset.X.numpy()\n",
    "X_prod_flat = X_prod.reshape(-1, config['data']['n_features'])\n",
    "\n",
    "drift_detector.monitor(X_prod_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Instructions\n",
    "\n",
    "### Start API Server:\n",
    "```bash\n",
    "cd deploy/api\n",
    "uvicorn app:app --host 0.0.0.0 --port 5000 --reload\n",
    "```\n",
    "\n",
    "### Test API:\n",
    "```bash\n",
    "curl -X POST http://localhost:5000/predict \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"temperature\": [...100 values...],\n",
    "    \"vibration\": [...100 values...],\n",
    "    \"pressure\": [...100 values...],\n",
    "    \"rpm\": [...100 values...]\n",
    "  }'\n",
    "```\n",
    "\n",
    "### MLflow UI:\n",
    "```bash\n",
    "mlflow ui --port 5001\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "logger.info(\"=\"*80)\n",
    "logger.info(\"PIPELINE COMPLETE ✓\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"Next steps:\")\n",
    "logger.info(\"1. Review MLflow UI: mlflow ui --port 5001\")\n",
    "logger.info(\"2. Deploy API: cd deploy/api && uvicorn app:app\")\n",
    "logger.info(\"3. Monitor production: Use drift_detector on incoming data\")\n",
    "logger.info(\"4. Retrain when drift detected or performance degrades\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
